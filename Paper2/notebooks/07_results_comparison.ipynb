{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 2: Results Comparison\n",
    "\n",
    "## Comprehensive Performance Analysis of All 5 Models\n",
    "\n",
    "This notebook:\n",
    "1. Loads results from all 5 trained models\n",
    "2. Creates comparison tables (matching Paper 2 Table 9 & Table 10)\n",
    "3. Generates comparative visualizations\n",
    "4. Identifies best model\n",
    "5. Analyzes trade-offs (accuracy vs computational cost)\n",
    "\n",
    "**Models Compared:**\n",
    "1. CNNs-without-Aug (Target: 99.22%)\n",
    "2. CNNs-with-Aug (Target: 99.61%)\n",
    "3. CNN-LSTM-with-Aug (Target: 99.92%) â­ BEST\n",
    "4. CNN-SVM-with-Aug (Target: 99.14%)\n",
    "5. VGG16-SVM-with-Aug (Target: 98.67%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 00_utils_and_config.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from all models\n",
    "results_all = {}\n",
    "model_names = [\"model1\", \"model2\", \"model3\", \"model4\", \"model5\"]\n",
    "model_display_names = [\n",
    "    \"CNNs-without-Aug\",\n",
    "    \"CNNs-with-Aug\",\n",
    "    \"CNN-LSTM-with-Aug\",\n",
    "    \"CNN-SVM-with-Aug\",\n",
    "    \"VGG16-SVM-with-Aug\"\n",
    "]\n",
    "\n",
    "print(\"Loading model results...\\n\")\n",
    "for model_name, display_name in zip(model_names, model_display_names):\n",
    "    result_file = CONFIG['results_path'] / f\"{model_name}_results.json\"\n",
    "    if result_file.exists():\n",
    "        with open(result_file) as f:\n",
    "            results_all[display_name] = json.load(f)\n",
    "        print(f\"âœ“ Loaded {display_name}\")\n",
    "    else:\n",
    "        print(f\"âš  {display_name} results not found (may need to run training notebook)\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(results_all)}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Comparison Table (Paper 2 Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe matching Paper 2 Table 9 & Table 10\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in results_all.items():\n",
    "    comparison_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": f\"{results.get('accuracy', 0)*100:.2f}%\",\n",
    "        \"Precision\": f\"{results.get('precision', 0)*100:.2f}%\",\n",
    "        \"Recall\": f\"{results.get('recall', 0)*100:.2f}%\",\n",
    "        \"F1-Score\": f\"{results.get('f1_score', 0)*100:.2f}%\",\n",
    "        \"Specificity\": f\"{results.get('specificity', 0)*100:.2f}%\",\n",
    "        \"Train Time (s)\": f\"{results.get('training_time_seconds', 0):.1f}\",\n",
    "        \"Test Time (ms)\": f\"{results.get('testing_time_ms', 0):.1f}\"\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON TABLE (Matching Paper 2 Table 9 & Table 10)\")\n",
    "print(\"=\"*100)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save to CSV\n",
    "df_comparison.to_csv(CONFIG['results_path'] / 'all_models_comparison.csv', index=False)\n",
    "print(\"\\nâœ“ Saved to all_models_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find model with highest accuracy\n",
    "if results_all:\n",
    "    accuracies = {name: res.get('accuracy', 0) for name, res in results_all.items()}\n",
    "    best_model = max(accuracies, key=accuracies.get)\n",
    "    best_accuracy = accuracies[best_model]\n",
    "    \n",
    "    print(f\"\\nâ­ BEST MODEL: {best_model}\")\n",
    "    print(f\"   Accuracy: {best_accuracy*100:.2f}%\")\n",
    "    print(f\"\\n   Expected (Paper 2): CNN-LSTM-with-Aug with 99.92%\")\n",
    "    \n",
    "    # Rankings\n",
    "    ranked = sorted(accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\nModel Rankings (by accuracy):\")\n",
    "    for i, (name, acc) in enumerate(ranked, 1):\n",
    "        print(f\"  {i}. {name}: {acc*100:.2f}%\")\n",
    "else:\n",
    "    print(\"\\nâš  No model results loaded. Please run training notebooks first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Performance Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_all:\n",
    "    # Bar chart comparing all metrics\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        values = [results_all[name].get(metric, 0)*100 for name in model_display_names if name in results_all]\n",
    "        labels = [name.split('-')[0][:10] for name in model_display_names if name in results_all]\n",
    "        \n",
    "        bars = ax.bar(range(len(labels)), values, color='steelblue', alpha=0.7)\n",
    "        ax.set_xticks(range(len(labels)))\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax.set_ylabel(f\"{metric.replace('_', ' ').title()} (%)\")\n",
    "        ax.set_title(f\"{metric.replace('_', ' ').title()} Comparison\")\n",
    "        ax.set_ylim([90, 101])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Annotate bars\n",
    "        for bar, val in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.3, \n",
    "                   f\"{val:.1f}\", ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Training time comparison\n",
    "    ax = axes[1, 2]\n",
    "    train_times = [results_all[name].get('training_time_seconds', 0) \n",
    "                   for name in model_display_names if name in results_all]\n",
    "    labels = [name.split('-')[0][:10] for name in model_display_names if name in results_all]\n",
    "    \n",
    "    bars = ax.bar(range(len(labels)), train_times, color='coral', alpha=0.7)\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Training Time (s)')\n",
    "    ax.set_title('Training Time Comparison')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Paper 2: All Models Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CONFIG['results_path'] / 'all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Comparison charts saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trade-off Analysis: Accuracy vs Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_all:\n",
    "    # Scatter plot: Accuracy vs Training Time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for name in model_display_names:\n",
    "        if name in results_all:\n",
    "            acc = results_all[name].get('accuracy', 0) * 100\n",
    "            time = results_all[name].get('training_time_seconds', 0)\n",
    "            plt.scatter(time, acc, s=200, alpha=0.6, label=name)\n",
    "            plt.annotate(name.split('-')[0], (time, acc), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    plt.xlabel('Training Time (seconds)', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('Accuracy vs Training Time Trade-off', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='lower right', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CONFIG['results_path'] / 'accuracy_vs_time_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Trade-off analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with Paper 2 Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_all:\n",
    "    # Compare achieved vs target accuracy\n",
    "    target_accuracies = {\n",
    "        'CNNs-without-Aug': 99.22,\n",
    "        'CNNs-with-Aug': 99.61,\n",
    "        'CNN-LSTM-with-Aug': 99.92,\n",
    "        'CNN-SVM-with-Aug': 99.14,\n",
    "        'VGG16-SVM-with-Aug': 98.67\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON WITH PAPER 2 TARGET ACCURACIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_name in model_display_names:\n",
    "        if model_name in results_all:\n",
    "            achieved = results_all[model_name].get('accuracy', 0) * 100\n",
    "            target = target_accuracies.get(model_name, 0)\n",
    "            diff = achieved - target\n",
    "            status = \"âœ“\" if diff >= -0.5 else \"âš \"\n",
    "            \n",
    "            print(f\"{status} {model_name}:\")\n",
    "            print(f\"   Achieved: {achieved:.2f}%\")\n",
    "            print(f\"   Target:   {target:.2f}%\")\n",
    "            print(f\"   Diff:     {diff:+.2f}%\\n\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PAPER 2 IMPLEMENTATION - FINAL SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "if results_all:\n",
    "    print(f\"\\nâœ“ Successfully trained and evaluated {len(results_all)}/5 models\")\n",
    "    \n",
    "    # Load dataset info\n",
    "    try:\n",
    "        train_size = len(np.load(CONFIG['processed_data_path'] / 'y_train.npy'))\n",
    "        test_size = len(np.load(CONFIG['processed_data_path'] / 'y_test.npy'))\n",
    "        print(f\"âœ“ Dataset: OASIS-2 ({train_size} train, {test_size} test slices)\")\n",
    "    except:\n",
    "        print(f\"âœ“ Dataset: OASIS-2 MRI brain scans\")\n",
    "    \n",
    "    print(f\"\\nðŸ† BEST MODEL: {best_model}\")\n",
    "    print(f\"   Accuracy: {best_accuracy*100:.2f}%\")\n",
    "    print(f\"   Paper 2 Best: CNN-LSTM-with-Aug (99.92%)\")\n",
    "    \n",
    "    print(f\"\\nModel Performance Summary (sorted by accuracy):\")\n",
    "    for i, (name, acc) in enumerate(ranked, 1):\n",
    "        target = target_accuracies.get(name, 0)\n",
    "        status = \"âœ“\" if acc*100 >= target * 0.98 else \"âš \"\n",
    "        print(f\"  {i}. {status} {name}: {acc*100:.2f}% (target: {target:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nâœ“ All results saved to: {CONFIG['results_path']}\")\n",
    "    print(f\"âœ“ Models saved to: {CONFIG['saved_models_path']}\")\n",
    "    print(f\"âœ“ Comparison table: all_models_comparison.csv\")\n",
    "    print(f\"âœ“ Visualizations: all_models_comparison.png, accuracy_vs_time_tradeoff.png\")\n",
    "else:\n",
    "    print(\"\\nâš  No model results available yet\")\n",
    "    print(\"   Please run training notebooks (02-06) first\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"Paper 2 methodology successfully implemented!\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
