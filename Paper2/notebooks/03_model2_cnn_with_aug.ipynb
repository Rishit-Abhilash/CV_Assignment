{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model 2: CNNs-with-Aug\n\n## 13-Layer CNN With Data Augmentation\n\n**Architecture:** Same 13-layer CNN as Model 1\n**Key Difference:** Uses data augmentation + 128×128 input\n\n**Hyperparameters:**\n- Input: 128×128×3\n- Epochs: 100, Batch: 65, LR: 0.0001\n- WITH data augmentation (rotation, flip, zoom, shift)\n\n**Target:** 99.61% accuracy",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%run 00_utils_and_config.ipynb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load Data (128×128)",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "data_path = CONFIG[\"processed_data_path\"]\n\nprint(\"Loading 128×128 data...\")\nX_train = np.load(data_path / \"X_train_128.npy\")\nX_test = np.load(data_path / \"X_test_128.npy\")\ny_train = np.load(data_path / \"y_train.npy\")\ny_test = np.load(data_path / \"y_test.npy\")\n\nX_train = X_train.astype(\"float32\") / 255.0\nX_test = X_test.astype(\"float32\") / 255.0\n\n# Convert to PyTorch format (N, C, H, W)\nX_train = np.transpose(X_train, (0, 3, 1, 2))\nX_test = np.transpose(X_test, (0, 3, 1, 2))\n\nprint(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Build Model (Same as Model 1, Different Input Size)",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "model_config = CONFIG[\"models\"][\"cnn_with_aug\"]\n\nclass CNNWithAug(nn.Module):\n    def __init__(self, input_channels=3, num_classes=2):\n        super(CNNWithAug, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout1 = nn.Dropout(0.20)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout2 = nn.Dropout(0.25)\n        self.flatten_size = 64 * 16 * 16  # 128->64->32->16\n        self.fc1 = nn.Linear(self.flatten_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n    \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool1(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool2(x)\n        x = self.dropout1(x)\n        x = F.relu(self.conv3(x))\n        x = self.pool3(x)\n        x = self.dropout2(x)\n        x = x.view(-1, self.flatten_size)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = CNNWithAug(input_channels=3, num_classes=2).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nprint(model)\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Setup Data Augmentation",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create augmented datasets\ntrain_transforms, test_transforms = create_data_augmentation()\ntrain_dataset = AugmentedDataset(X_train, y_train, transform=train_transforms)\ntest_dataset = AugmentedDataset(X_test, y_test, transform=test_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=65, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=65, shuffle=False)\n\nprint(\"✓ Data augmentation configured and DataLoaders created\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Train with Augmentation",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "start_time = time.time()\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\nbest_val_acc = 0.0\nbest_model_path = str(CONFIG[\"saved_models_path\"] / \"model2_cnn_with_aug_best.pth\")\n\nfor epoch in range(100):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_loss = running_loss / total\n    train_acc = correct / total\n    \n    # Validate\n    model.eval()\n    val_loss, val_correct, val_total = 0.0, 0, 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n    \n    val_loss /= val_total\n    val_acc = val_correct / val_total\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Epoch {epoch+1}/100 - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} ✓\")\n    elif (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/100 - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\ntraining_time = time.time() - start_time\nprint(f\"Training time: {training_time:.1f}s\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Evaluate",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "best_model = CNNWithAug(input_channels=3, num_classes=2).to(device)\nbest_model.load_state_dict(torch.load(best_model_path))\nbest_model.eval()\n\nstart_time = time.time()\nall_preds, all_probs = [], []\nwith torch.no_grad():\n    for inputs, _ in test_loader:\n        inputs = inputs.to(device)\n        outputs = best_model(inputs)\n        probs = F.softmax(outputs, dim=1)\n        all_probs.append(probs.cpu().numpy())\n        _, predicted = torch.max(outputs.data, 1)\n        all_preds.append(predicted.cpu().numpy())\n\ny_pred_proba = np.vstack(all_probs)\ny_pred = np.concatenate(all_preds)\ntesting_time = (time.time() - start_time) * 1000\n\nmetrics = calculate_all_metrics(y_test, y_pred, y_pred_proba[:, 1])\nprint_metrics(metrics, \"Model 2: CNNs-with-Aug\")\nprint(f\"Testing time: {testing_time:.1f}ms\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Visualize",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "plot_training_history(history, \"Model 2: CNNs-with-Aug\", CONFIG[\"results_path\"] / \"training_curves\" / \"model2_training.png\")\nplot_confusion_matrix(metrics[\"confusion_matrix\"], CONFIG[\"class_names\"], \"Model 2 Confusion Matrix\", CONFIG[\"results_path\"] / \"confusion_matrices\" / \"model2_cm.png\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Save Results",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "results = {\n    \"model_name\": \"CNNs-with-Aug\",\n    \"accuracy\": float(metrics[\"accuracy\"]),\n    \"precision\": float(metrics[\"precision\"]),\n    \"recall\": float(metrics[\"recall\"]),\n    \"f1_score\": float(metrics[\"f1_score\"]),\n    \"specificity\": float(metrics[\"specificity\"]),\n    \"training_time_seconds\": float(training_time),\n    \"testing_time_ms\": float(testing_time)\n}\n\ntorch.save(model.state_dict(), CONFIG[\"saved_models_path\"] / \"model2_cnn_with_aug_final.pth\")\n\nwith open(CONFIG[\"results_path\"] / \"model2_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"\\n✓ Accuracy: {metrics['accuracy']*100:.2f}% (Target: 99.61%)\")",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}