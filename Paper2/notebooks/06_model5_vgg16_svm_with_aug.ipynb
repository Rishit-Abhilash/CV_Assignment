{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model 5: VGG16-SVM-with-Aug\n\n## Transfer Learning: Pre-trained VGG16 + SVM\n\n**Architecture:** VGG16 (remove top) + SVM classifier\n- Uses pre-trained ImageNet weights\n- Linear SVM kernel\n- Input: 224×224×3\n\n**Target:** 98.67% accuracy",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%run 00_utils_and_config.ipynb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "X_train = np.load(CONFIG[\"processed_data_path\"] / \"X_train_224.npy\").astype(\"float32\")\nX_test = np.load(CONFIG[\"processed_data_path\"] / \"X_test_224.npy\").astype(\"float32\")\ny_train = np.load(CONFIG[\"processed_data_path\"] / \"y_train.npy\")\ny_test = np.load(CONFIG[\"processed_data_path\"] / \"y_test.npy\")\n\n# Convert to PyTorch format (N, C, H, W)\nX_train = np.transpose(X_train, (0, 3, 1, 2)) / 255.0\nX_test = np.transpose(X_test, (0, 3, 1, 2)) / 255.0\n\n# Normalize using ImageNet stats for VGG16\nmean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\nX_train = (torch.from_numpy(X_train).float() - mean) / std\nX_test = (torch.from_numpy(X_test).float() - mean) / std\n\nprint(f\"Data ready: {X_train.shape}, {X_test.shape}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Build VGG16 + SVM",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load pre-trained VGG16 from torchvision\nbase_model = torchvision_models.vgg16(pretrained=True)\n\n# Freeze feature extractor\nfor param in base_model.features.parameters():\n    param.requires_grad = False\n\n# Replace classifier with SVM-like layer\nclass VGG16SVM(nn.Module):\n    def __init__(self, base_model, num_classes=2):\n        super(VGG16SVM, self).__init__()\n        self.features = base_model.features\n        self.avgpool = base_model.avgpool\n        # VGG16 outputs 512 * 7 * 7 = 25088 features\n        self.flatten_size = 25088\n        self.fc = nn.Linear(self.flatten_size, num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(-1, self.flatten_size)\n        x = self.fc(x)\n        return x\n\nmodel = VGG16SVM(base_model, num_classes=2).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.0001, weight_decay=0.01)  # Only train classifier\n\nprint(f\"Total params: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create datasets and loaders\ntrain_dataset = TensorDataset(X_train, torch.from_numpy(y_train).long())\ntest_dataset = TensorDataset(X_test, torch.from_numpy(y_test).long())\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nstart_time = time.time()\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\nbest_val_acc = 0.0\nbest_model_path = str(CONFIG[\"saved_models_path\"] / \"model5_vgg16_svm_best.pth\")\n\nfor epoch in range(10):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_loss = running_loss / total\n    train_acc = correct / total\n    \n    model.eval()\n    val_loss, val_correct, val_total = 0.0, 0, 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n    \n    val_loss /= val_total\n    val_acc = val_correct / val_total\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n    \n    print(f\"Epoch {epoch+1}/10 - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\ntraining_time = time.time() - start_time",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Evaluate",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "best_model = VGG16SVM(base_model, num_classes=2).to(device)\nbest_model.load_state_dict(torch.load(best_model_path))\nbest_model.eval()\n\nstart_time = time.time()\nall_preds, all_probs = [], []\nwith torch.no_grad():\n    for inputs, _ in test_loader:\n        inputs = inputs.to(device)\n        outputs = best_model(inputs)\n        probs = F.softmax(outputs, dim=1)\n        all_probs.append(probs.cpu().numpy())\n        _, predicted = torch.max(outputs.data, 1)\n        all_preds.append(predicted.cpu().numpy())\n\ny_pred_proba = np.vstack(all_probs)\ny_pred = np.concatenate(all_preds)\ntesting_time = (time.time() - start_time) * 1000\n\nmetrics = calculate_all_metrics(y_test, y_pred, y_pred_proba[:, 1])\nprint_metrics(metrics, \"Model 5: VGG16-SVM\")\nprint(f\"Accuracy: {metrics['accuracy']*100:.2f}% (Target: 98.67%)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "plot_training_history(history, \"Model 5: VGG16-SVM\", CONFIG[\"results_path\"] / \"training_curves\" / \"model5_training.png\")\ntorch.save(model.state_dict(), CONFIG[\"saved_models_path\"] / \"model5_vgg16_svm_final.pth\")\nresults = {\"model_name\": \"VGG16-SVM-with-Aug\", \"accuracy\": float(metrics[\"accuracy\"]), \"precision\": float(metrics[\"precision\"]), \"recall\": float(metrics[\"recall\"]), \"f1_score\": float(metrics[\"f1_score\"]), \"training_time_seconds\": float(training_time), \"testing_time_ms\": float(testing_time)}\nwith open(CONFIG[\"results_path\"] / \"model5_results.json\", \"w\") as f: json.dump(results, f, indent=2)\nprint(\"✓ VGG16-SVM complete\")",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}