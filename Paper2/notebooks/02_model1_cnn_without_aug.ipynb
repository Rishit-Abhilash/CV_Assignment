{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: CNNs-without-Aug\n",
    "\n",
    "## 13-Layer CNN Without Data Augmentation\n",
    "\n",
    "**Architecture:** 13 layers total\n",
    "- Conv2D(16) → MaxPool → Conv2D(32) → MaxPool → Dropout(0.25)\n",
    "- Conv2D(64) → MaxPool → Dropout(0.20) → Flatten\n",
    "- Dense(128) → Dense(64) → Dense(2, Softmax)\n",
    "\n",
    "**Hyperparameters (from Paper 2):**\n",
    "- Input: 224×224×3\n",
    "- Epochs: 100\n",
    "- Batch size: 30\n",
    "- Learning rate: 0.0001\n",
    "- Optimizer: Adam\n",
    "- Loss: binary_crossentropy\n",
    "- NO data augmentation\n",
    "\n",
    "**Target Performance:** 99.22% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load utilities\n",
    "%run 00_utils_and_config.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load 224x224 data (for this model)\ndata_path = CONFIG['processed_data_path']\n\nprint(\"Loading preprocessed data...\")\nX_train = np.load(data_path / 'X_train_224.npy')\nX_test = np.load(data_path / 'X_test_224.npy')\ny_train = np.load(data_path / 'y_train.npy')\ny_test = np.load(data_path / 'y_test.npy')\n\nprint(f\"\\nData shapes:\")\nprint(f\"  X_train: {X_train.shape}\")\nprint(f\"  X_test:  {X_test.shape}\")\nprint(f\"  y_train: {y_train.shape}\")\nprint(f\"  y_test:  {y_test.shape}\")\n\n# Normalize to [0, 1] and convert to PyTorch format (C, H, W)\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\n\n# Convert from (N, H, W, C) to (N, C, H, W) for PyTorch\nX_train = np.transpose(X_train, (0, 3, 1, 2))\nX_test = np.transpose(X_test, (0, 3, 1, 2))\n\nprint(f\"\\n✓ Data normalized and converted to PyTorch format\")\nprint(f\"  X_train: {X_train.shape} (N, C, H, W)\")\nprint(f\"  X_test:  {X_test.shape} (N, C, H, W)\")\n\n# Create PyTorch tensors\nX_train_tensor = torch.from_numpy(X_train).float()\nX_test_tensor = torch.from_numpy(X_test).float()\ny_train_tensor = torch.from_numpy(y_train).long()\ny_test_tensor = torch.from_numpy(y_test).long()\n\n# Create DataLoaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\nbatch_size = CONFIG['models']['cnn_without_aug']['batch_size']\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"\\n✓ DataLoaders created (batch_size={batch_size})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get model config\nmodel_config = CONFIG['models']['cnn_without_aug']\n\nclass CNNWithoutAug(nn.Module):\n    \"\"\"\n    13-layer CNN architecture exactly as specified in Paper 2.\n    \n    Architecture:\n    - Input (3, 224, 224)\n    - Conv2D(16, 3×3, ReLU) → MaxPooling2D(2×2)\n    - Conv2D(32, 3×3, ReLU) → MaxPooling2D(2×2) → Dropout(0.25)\n    - Conv2D(64, 3×3, ReLU) → MaxPooling2D(2×2) → Dropout(0.20)\n    - Flatten\n    - Dense(128, ReLU) → Dense(64, ReLU)\n    - Dense(2, Softmax)\n    \"\"\"\n    \n    def __init__(self, input_channels=3, num_classes=2):\n        super(CNNWithoutAug, self).__init__()\n        \n        # First convolutional block\n        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Second convolutional block\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout1 = nn.Dropout(0.25)\n        \n        # Third convolutional block\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout2 = nn.Dropout(0.20)\n        \n        # Calculate flattened size after convolutions\n        # Input: 224x224 -> conv1+pool1: 112x112 -> conv2+pool2: 56x56 -> conv3+pool3: 28x28\n        self.flatten_size = 64 * 28 * 28\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(self.flatten_size, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, num_classes)\n        \n    def forward(self, x):\n        # First convolutional block\n        x = F.relu(self.conv1(x))\n        x = self.pool1(x)\n        \n        # Second convolutional block\n        x = F.relu(self.conv2(x))\n        x = self.pool2(x)\n        x = self.dropout1(x)\n        \n        # Third convolutional block\n        x = F.relu(self.conv3(x))\n        x = self.pool3(x)\n        x = self.dropout2(x)\n        \n        # Flatten\n        x = x.view(-1, self.flatten_size)\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        \n        return x\n\n# Build model\nmodel = CNNWithoutAug(input_channels=3, num_classes=2).to(device)\n\n# Display architecture\nprint(model)\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=model_config['learning_rate'])\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7, verbose=True\n)\n\nprint(f\"\\n✓ Model setup complete:\")\nprint(f\"  Optimizer: Adam (lr={model_config['learning_rate']})\")\nprint(f\"  Loss: CrossEntropyLoss\")\nprint(f\"  Scheduler: ReduceLROnPlateau\")\nprint(f\"  Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training helper functions\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Statistics\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    \n    return epoch_loss, epoch_acc\n\n\ndef validate_epoch(model, val_loader, criterion, device):\n    \"\"\"Validate for one epoch.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    \n    return epoch_loss, epoch_acc\n\nprint(f\"Training Model 1: CNNs-without-Aug\")\nprint(f\"Epochs: {model_config['epochs']}\")\nprint(f\"Batch size: {model_config['batch_size']}\")\nprint(f\"Target accuracy: {model_config['target_accuracy']*100:.2f}%\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train model and track time\nstart_time = time.time()\n\n# Split train data for validation (80% train, 20% val)\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n\ntrain_loader_split = DataLoader(train_subset, batch_size=model_config['batch_size'], shuffle=True)\nval_loader = DataLoader(val_subset, batch_size=model_config['batch_size'], shuffle=False)\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_acc': []\n}\n\nbest_val_acc = 0.0\nbest_model_path = str(CONFIG['saved_models_path'] / 'model1_cnn_without_aug_best.pth')\n\n# Training loop\nfor epoch in range(model_config['epochs']):\n    # Train\n    train_loss, train_acc = train_epoch(model, train_loader_split, criterion, optimizer, device)\n    \n    # Validate\n    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n    \n    # Update learning rate\n    scheduler.step(val_loss)\n    \n    # Save history\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    \n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Epoch {epoch+1}/{model_config['epochs']} - \"\n              f\"Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} - Best model saved\")\n    elif (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{model_config['epochs']} - \"\n              f\"Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\ntraining_time = time.time() - start_time\n\nprint(f\"\\n✓ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\nprint(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_history(\n",
    "    history, \n",
    "    model_name=\"Model 1: CNNs-without-Aug\",\n",
    "    save_path=CONFIG['results_path'] / 'training_curves' / 'model1_training_curves.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best model\nbest_model = CNNWithoutAug(input_channels=3, num_classes=2).to(device)\nbest_model.load_state_dict(torch.load(best_model_path))\nbest_model.eval()\n\nprint(\"Loaded best model from training\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Make predictions and track time\nstart_time = time.time()\n\nall_preds = []\nall_probs = []\n\nwith torch.no_grad():\n    for inputs, _ in test_loader:\n        inputs = inputs.to(device)\n        outputs = best_model(inputs)\n        probs = F.softmax(outputs, dim=1)\n        \n        all_probs.append(probs.cpu().numpy())\n        _, predicted = torch.max(outputs.data, 1)\n        all_preds.append(predicted.cpu().numpy())\n\ny_pred_proba = np.vstack(all_probs)\ny_pred = np.concatenate(all_preds)\n\ntesting_time = (time.time() - start_time) * 1000  # Convert to milliseconds\ntesting_time_per_sample = testing_time / len(X_test_tensor)\n\nprint(f\"Testing time: {testing_time:.2f} ms total\")\nprint(f\"Testing time per sample: {testing_time_per_sample:.4f} ms\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "metrics = calculate_all_metrics(y_test, y_pred, y_pred_proba[:, 1])\n",
    "\n",
    "# Print metrics\n",
    "print_metrics(metrics, model_name=\"Model 1: CNNs-without-Aug\")\n",
    "\n",
    "# Add timing information\n",
    "metrics['training_time_seconds'] = training_time\n",
    "metrics['testing_time_ms'] = testing_time\n",
    "metrics['testing_time_per_sample_ms'] = testing_time_per_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    metrics['confusion_matrix'],\n",
    "    CONFIG['class_names'],\n",
    "    title=\"Model 1: CNNs-without-Aug - Confusion Matrix\",\n",
    "    save_path=CONFIG['results_path'] / 'confusion_matrices' / 'model1_confusion_matrix.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "plot_roc_curve(\n",
    "    metrics,\n",
    "    model_name=\"Model 1: CNNs-without-Aug\",\n",
    "    save_path=CONFIG['results_path'] / 'model1_roc_curve.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save final model\ntorch.save(model.state_dict(), CONFIG['saved_models_path'] / 'model1_cnn_without_aug_final.pth')\n\n# Save metrics\nresults = {\n    'model_name': 'CNNs-without-Aug',\n    'accuracy': float(metrics['accuracy']),\n    'precision': float(metrics['precision']),\n    'recall': float(metrics['recall']),\n    'f1_score': float(metrics['f1_score']),\n    'specificity': float(metrics['specificity']),\n    'auc': float(metrics.get('auc', 0)),\n    'training_time_seconds': float(training_time),\n    'testing_time_ms': float(testing_time),\n    'confusion_matrix': metrics['confusion_matrix'].tolist(),\n    'hyperparameters': model_config\n}\n\nwith open(CONFIG['results_path'] / 'model1_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\n✓ Model and results saved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"MODEL 1: CNNs-without-Aug - FINAL SUMMARY\")\nprint(\"=\"*80)\nprint(f\"\\nArchitecture: 13-layer CNN (PyTorch)\")\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Target parameters (Paper 2): {model_config['total_params']:,}\")\nprint(f\"\\nPerformance:\")\nprint(f\"  Accuracy:     {metrics['accuracy']*100:.2f}% (Target: {model_config['target_accuracy']*100:.2f}%)\")\nprint(f\"  Precision:    {metrics['precision']*100:.2f}%\")\nprint(f\"  Recall:       {metrics['recall']*100:.2f}%\")\nprint(f\"  F1-Score:     {metrics['f1_score']*100:.2f}%\")\nprint(f\"  Specificity:  {metrics['specificity']*100:.2f}%\")\nprint(f\"  AUC:          {metrics.get('auc', 0):.4f}\")\nprint(f\"\\nTiming:\")\nprint(f\"  Training time: {training_time:.1f}s (Target: ~224s)\")\nprint(f\"  Testing time:  {testing_time:.1f}ms (Target: ~4ms)\")\nprint(\"\\n\" + \"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}