{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 2: Data Preparation\n",
    "\n",
    "## Extract 2D Slices from OASIS-2 3D NIfTI Volumes\n",
    "\n",
    "This notebook:\n",
    "1. Loads OASIS demographics (CDR scores for labeling)\n",
    "2. Extracts 2D slices from 3D MRI volumes (targeting ~6,400 slices)\n",
    "3. Preprocesses slices (resize, normalize, RGB conversion)\n",
    "4. Creates binary labels (Demented vs Non-Demented)\n",
    "5. Performs train-test split (80/20)\n",
    "6. Saves processed data for model training\n",
    "\n",
    "**Dataset:** OASIS-2 Raw MRI (1,367 3D volumes → ~6,400 2D slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Utilities and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Device: cuda\n",
      "Configuration loaded successfully!\n",
      "\n",
      "Base path: C:\\Users\\rishi\\CV_Assignment\\Paper2\n",
      "Raw data path: C:\\Users\\rishi\\CV_Assignment\\Paper2\\Raw_Data\n",
      "Number of models: 5\n",
      "Data processing functions loaded successfully!\n",
      "Evaluation metrics functions loaded successfully!\n",
      "Visualization functions loaded successfully!\n",
      "Data augmentation setup loaded successfully!\n",
      "\n",
      "================================================================================\n",
      "PAPER 2 UTILITIES AND CONFIGURATION - SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ All libraries imported successfully\n",
      "✓ Configuration parameters loaded\n",
      "✓ Data processing functions defined\n",
      "✓ Evaluation metrics functions defined\n",
      "✓ Visualization functions defined\n",
      "✓ Data augmentation configured\n",
      "\n",
      "Ready to proceed with:\n",
      "  - Notebook 01: Data Preparation\n",
      "  - Notebooks 02-06: Model Implementations\n",
      "  - Notebook 07: Results Comparison\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the utilities notebook to load all functions and config\n",
    "%run 00_utils_and_config.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load OASIS Demographics and Create Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading demographics from: ..\\Raw_Data\\OASIS_demographic.xlsx\n",
      "\n",
      "Demographics loaded: 373 records\n",
      "\n",
      "First 5 records:\n",
      "  Subject ID         MRI ID        Group  Visit  MR Delay M/F Hand  Age  EDUC  \\\n",
      "0  OAS2_0001  OAS2_0001_MR1  Nondemented      1         0   M    R   87    14   \n",
      "1  OAS2_0001  OAS2_0001_MR2  Nondemented      2       457   M    R   88    14   \n",
      "2  OAS2_0002  OAS2_0002_MR1     Demented      1         0   M    R   75    12   \n",
      "3  OAS2_0002  OAS2_0002_MR2     Demented      2       560   M    R   76    12   \n",
      "4  OAS2_0002  OAS2_0002_MR3     Demented      3      1895   M    R   80    12   \n",
      "\n",
      "   SES  MMSE  CDR         eTIV      nWBV       ASF  \n",
      "0  2.0  27.0  0.0  1986.550000  0.696106  0.883440  \n",
      "1  2.0  30.0  0.0  2004.479526  0.681062  0.875539  \n",
      "2  NaN  23.0  0.5  1678.290000  0.736336  1.045710  \n",
      "3  NaN  28.0  0.5  1737.620000  0.713402  1.010000  \n",
      "4  NaN  22.0  0.5  1697.911134  0.701236  1.033623  \n",
      "\n",
      "Columns: ['Subject ID', 'MRI ID', 'Group', 'Visit', 'MR Delay', 'M/F', 'Hand', 'Age', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF']\n"
     ]
    }
   ],
   "source": [
    "# Load demographics file\n",
    "demographics_path = CONFIG['demographics_file']\n",
    "print(f\"Loading demographics from: {demographics_path}\\n\")\n",
    "\n",
    "df_demographics = load_demographics(demographics_path)\n",
    "print(f\"Demographics loaded: {len(df_demographics)} records\\n\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 records:\")\n",
    "print(df_demographics.head())\n",
    "\n",
    "# Display column names\n",
    "print(f\"\\nColumns: {list(df_demographics.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total records with valid CDR scores: 373\n",
      "\n",
      "Class distribution:\n",
      "Binary_Label\n",
      "0    206\n",
      "1    167\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  0 (Non-Demented): 206\n",
      "  1 (Demented):     167\n"
     ]
    }
   ],
   "source": [
    "# Extract relevant columns (adjust column names based on actual Excel file)\n",
    "# Typical OASIS-2 columns: 'Subject ID', 'MRI ID', 'Visit', 'CDR', 'Age', 'MMSE', etc.\n",
    "\n",
    "# Create binary labels from CDR scores\n",
    "# CDR >= 0.5 = Demented (1), CDR = 0 = Non-Demented (0)\n",
    "if 'CDR' in df_demographics.columns:\n",
    "    df_demographics['Binary_Label'] = df_demographics['CDR'].apply(\n",
    "        lambda x: get_binary_label(x, threshold=CONFIG['cdr_threshold'])\n",
    "    )\n",
    "    \n",
    "    # Remove rows with missing CDR scores\n",
    "    df_demographics = df_demographics.dropna(subset=['Binary_Label'])\n",
    "    df_demographics['Binary_Label'] = df_demographics['Binary_Label'].astype(int)\n",
    "    \n",
    "    print(f\"\\nTotal records with valid CDR scores: {len(df_demographics)}\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df_demographics['Binary_Label'].value_counts())\n",
    "    print(f\"\\n  0 (Non-Demented): {(df_demographics['Binary_Label'] == 0).sum()}\")\n",
    "    print(f\"  1 (Demented):     {(df_demographics['Binary_Label'] == 1).sum()}\")\n",
    "else:\n",
    "    print(\"\\nWarning: 'CDR' column not found. Please adjust column name.\")\n",
    "    print(f\"Available columns: {list(df_demographics.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label mapping created for 373 MRI sessions\n",
      "\n",
      "Sample mappings:\n",
      "  OAS2_0001_MR1: 0 (Non-Demented)\n",
      "  OAS2_0001_MR2: 0 (Non-Demented)\n",
      "  OAS2_0002_MR1: 1 (Demented)\n",
      "  OAS2_0002_MR2: 1 (Demented)\n",
      "  OAS2_0002_MR3: 1 (Demented)\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from MRI ID to label\n",
    "# OASIS-2 format: OAS2_XXXX_MRY\n",
    "if 'MRI ID' in df_demographics.columns:\n",
    "    label_mapping = dict(zip(df_demographics['MRI ID'], df_demographics['Binary_Label']))\n",
    "    print(f\"\\nLabel mapping created for {len(label_mapping)} MRI sessions\")\n",
    "    print(f\"\\nSample mappings:\")\n",
    "    for mri_id, label in list(label_mapping.items())[:5]:\n",
    "        label_name = CONFIG['class_names'][label]\n",
    "        print(f\"  {mri_id}: {label} ({label_name})\")\n",
    "else:\n",
    "    print(\"\\nWarning: 'MRI ID' column not found. Please adjust column name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scan Raw Data Directory and Collect NIfTI Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning raw data directories...\n",
      "\n",
      "Scanning: OAS2_RAW_PART1\n",
      "  Found 771 NIfTI header files\n",
      "Scanning: OAS2_RAW_PART2\n",
      "  Found 596 NIfTI header files\n",
      "\n",
      "Total NIfTI files found: 1367\n",
      "\n",
      "Sample file paths:\n",
      "  1. ..\\Raw_Data\\OAS2_RAW_PART1\\OAS2_0001_MR1\\RAW\\mpr-1.nifti.hdr\n",
      "  2. ..\\Raw_Data\\OAS2_RAW_PART1\\OAS2_0001_MR1\\RAW\\mpr-2.nifti.hdr\n",
      "  3. ..\\Raw_Data\\OAS2_RAW_PART1\\OAS2_0001_MR1\\RAW\\mpr-3.nifti.hdr\n"
     ]
    }
   ],
   "source": [
    "# Scan both OAS2_RAW_PART1 and OAS2_RAW_PART2 directories\n",
    "raw_data_path = CONFIG['raw_data_path']\n",
    "part1_path = raw_data_path / 'OAS2_RAW_PART1'\n",
    "part2_path = raw_data_path / 'OAS2_RAW_PART2'\n",
    "\n",
    "print(f\"Scanning raw data directories...\\n\")\n",
    "\n",
    "# Collect all .hdr files (NIfTI headers)\n",
    "nifti_files = []\n",
    "\n",
    "for part_path in [part1_path, part2_path]:\n",
    "    if part_path.exists():\n",
    "        print(f\"Scanning: {part_path.name}\")\n",
    "        # Find all mpr-*.nifti.hdr files in RAW subdirectories\n",
    "        hdr_files = list(part_path.rglob('RAW/mpr-*.nifti.hdr'))\n",
    "        nifti_files.extend(hdr_files)\n",
    "        print(f\"  Found {len(hdr_files)} NIfTI header files\")\n",
    "\n",
    "print(f\"\\nTotal NIfTI files found: {len(nifti_files)}\")\n",
    "\n",
    "# Sample file paths\n",
    "print(f\"\\nSample file paths:\")\n",
    "for i, file_path in enumerate(nifti_files[:3]):\n",
    "    print(f\"  {i+1}. {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching NIfTI files to labels...\n",
      "Total NIfTI files to process: 1367\n",
      "Total MRI IDs in label mapping: 373\n",
      "\n",
      "Matching complete!\n",
      "  Matched: 1367\n",
      "  Unmatched: 0\n",
      "\n",
      "Sample matches:\n",
      "  OAS2_0001_MR1/.../mpr-1.nifti.hdr -> OAS2_0001_MR1 -> 0 (Non-Demented)\n",
      "  OAS2_0001_MR1/.../mpr-2.nifti.hdr -> OAS2_0001_MR1 -> 0 (Non-Demented)\n",
      "  OAS2_0001_MR1/.../mpr-3.nifti.hdr -> OAS2_0001_MR1 -> 0 (Non-Demented)\n",
      "  OAS2_0001_MR2/.../mpr-1.nifti.hdr -> OAS2_0001_MR2 -> 0 (Non-Demented)\n",
      "  OAS2_0001_MR2/.../mpr-2.nifti.hdr -> OAS2_0001_MR2 -> 0 (Non-Demented)\n",
      "\n",
      "NIfTI files with valid labels: 1367\n",
      "\n",
      "Class distribution in matched files:\n",
      "  Non-Demented (0): 753 volumes\n",
      "  Demented (1):     614 volumes\n"
     ]
    }
   ],
   "source": [
    "# Extract MRI session IDs from file paths\n",
    "# Path format: .../OAS2_XXXX_MRY/RAW/mpr-N.nifti.hdr\n",
    "def extract_mri_id_from_path(file_path):\n",
    "    \"\"\"Extract MRI session ID (OAS2_XXXX_MRY) from file path.\"\"\"\n",
    "    # Convert to Path object if it isn't already\n",
    "    path_obj = Path(file_path) if not isinstance(file_path, Path) else file_path\n",
    "    parts = path_obj.parts\n",
    "    for part in parts:\n",
    "        if part.startswith('OAS2_') and '_MR' in part:\n",
    "            return part\n",
    "    return None\n",
    "\n",
    "# Create list of (file_path, mri_id, label) tuples\n",
    "file_label_pairs = []\n",
    "\n",
    "print(f\"Matching NIfTI files to labels...\")\n",
    "print(f\"Total NIfTI files to process: {len(nifti_files)}\")\n",
    "print(f\"Total MRI IDs in label mapping: {len(label_mapping)}\\n\")\n",
    "\n",
    "# Debug: Show sample matching\n",
    "matched_count = 0\n",
    "unmatched_count = 0\n",
    "sample_matches = []\n",
    "\n",
    "for file_path in nifti_files:\n",
    "    mri_id = extract_mri_id_from_path(file_path)\n",
    "    \n",
    "    if mri_id and mri_id in label_mapping:\n",
    "        label = label_mapping[mri_id]\n",
    "        file_label_pairs.append((file_path, mri_id, label))\n",
    "        matched_count += 1\n",
    "        \n",
    "        # Store first 5 matches as samples\n",
    "        if len(sample_matches) < 5:\n",
    "            sample_matches.append((str(file_path), mri_id, label))\n",
    "    else:\n",
    "        unmatched_count += 1\n",
    "\n",
    "print(f\"Matching complete!\")\n",
    "print(f\"  Matched: {matched_count}\")\n",
    "print(f\"  Unmatched: {unmatched_count}\")\n",
    "print(f\"\\nSample matches:\")\n",
    "for path, mri_id, label in sample_matches:\n",
    "    label_name = CONFIG['class_names'][label]\n",
    "    # Show just the filename portion for readability\n",
    "    filename = Path(path).name\n",
    "    parent_dir = Path(path).parent.parent.name\n",
    "    print(f\"  {parent_dir}/.../{filename} -> {mri_id} -> {label} ({label_name})\")\n",
    "\n",
    "print(f\"\\nNIfTI files with valid labels: {len(file_label_pairs)}\")\n",
    "\n",
    "# Check class distribution\n",
    "if file_label_pairs:\n",
    "    labels_only = [label for _, _, label in file_label_pairs]\n",
    "    non_demented_count = labels_only.count(0)\n",
    "    demented_count = labels_only.count(1)\n",
    "    \n",
    "    print(f\"\\nClass distribution in matched files:\")\n",
    "    print(f\"  Non-Demented (0): {non_demented_count} volumes\")\n",
    "    print(f\"  Demented (1):     {demented_count} volumes\")\n",
    "else:\n",
    "    print(\"\\n⚠️  WARNING: No files matched! Check the following:\")\n",
    "    print(f\"  1. Sample file path: {nifti_files[0]}\")\n",
    "    print(f\"  2. Extracted MRI ID: {extract_mri_id_from_path(nifti_files[0])}\")\n",
    "    print(f\"  3. Sample label mapping keys: {list(label_mapping.keys())[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract 2D Slices from 3D Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target total slices: 6400\n",
      "Available volumes: 1367\n",
      "Slices per volume: 4\n",
      "Expected total slices: 5468\n",
      "\n",
      "Starting slice extraction... (this may take several minutes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate how many slices per volume we need to reach ~6,400 total slices\n",
    "target_total = CONFIG['target_total_images']\n",
    "num_volumes = len(file_label_pairs)\n",
    "\n",
    "# Check if we have any volumes to process\n",
    "if num_volumes == 0:\n",
    "    print(\"❌ ERROR: No volumes with valid labels found!\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  1. Are the NIfTI files in the correct directories?\")\n",
    "    print(\"  2. Do the file paths contain MRI IDs matching the demographics file?\")\n",
    "    print(\"  3. Run the previous cell again to see detailed matching information.\")\n",
    "    print(\"\\nCannot proceed with slice extraction.\")\n",
    "else:\n",
    "    slices_per_volume = max(1, target_total // num_volumes)\n",
    "    \n",
    "    print(f\"Target total slices: {target_total}\")\n",
    "    print(f\"Available volumes: {num_volumes}\")\n",
    "    print(f\"Slices per volume: {slices_per_volume}\")\n",
    "    print(f\"Expected total slices: {num_volumes * slices_per_volume}\\n\")\n",
    "    \n",
    "    print(\"Starting slice extraction... (this may take several minutes)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 1), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rishi\\CV_Assignment\\cvvenv\\Lib\\site-packages\\PIL\\Image.py:3285\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3285\u001b[39m     typemode, rawmode, color_modes = \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   3286\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyError\u001b[39m: ((1, 1, 1), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Preprocess each slice in both sizes\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m slice_idx, slice_2d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(slices_2d):\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Preprocess to 224x224\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     slice_224 = \u001b[43mpreprocess_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_rgb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     slices_224.append(slice_224)\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Preprocess to 128x128\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_28884\\1943793788.py:88\u001b[39m, in \u001b[36mpreprocess_slice\u001b[39m\u001b[34m(slice_2d, target_size, normalize, to_rgb)\u001b[39m\n\u001b[32m     85\u001b[39m     slice_normalized = np.zeros_like(slice_2d, dtype=np.uint8)\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Resize using PIL\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_normalized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m img_resized = img.resize(target_size, Image.BILINEAR)\n\u001b[32m     90\u001b[39m slice_resized = np.array(img_resized)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rishi\\CV_Assignment\\cvvenv\\Lib\\site-packages\\PIL\\Image.py:3289\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3287\u001b[39m         typekey_shape, typestr = typekey\n\u001b[32m   3288\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3289\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   3290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode != typemode \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m color_modes:\n",
      "\u001b[31mTypeError\u001b[39m: Cannot handle this data type: (1, 1, 1), |u1"
     ]
    }
   ],
   "source": [
    "# Extract slices in two sizes for different models\n",
    "# Size 224x224 for models 1, 4, 5\n",
    "# Size 128x128 for models 2, 3\n",
    "\n",
    "slices_224 = []\n",
    "slices_128 = []\n",
    "slice_labels = []\n",
    "slice_metadata = []  # Store (mri_id, slice_idx) for tracking\n",
    "\n",
    "# Process each volume\n",
    "failed_count = 0\n",
    "success_count = 0\n",
    "\n",
    "for idx, (file_path, mri_id, label) in enumerate(file_label_pairs):\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processing volume {idx + 1}/{num_volumes}...\")\n",
    "    \n",
    "    # Load 3D volume\n",
    "    volume = load_nifti_volume(file_path)\n",
    "    \n",
    "    if volume is None:\n",
    "        failed_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Extract representative 2D slices\n",
    "    slices_2d = extract_representative_slices(volume, num_slices=slices_per_volume, axis=2)\n",
    "    \n",
    "    if len(slices_2d) == 0:\n",
    "        failed_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Preprocess each slice in both sizes\n",
    "    for slice_idx, slice_2d in enumerate(slices_2d):\n",
    "        # Preprocess to 224x224\n",
    "        slice_224 = preprocess_slice(slice_2d, target_size=(224, 224), normalize=False, to_rgb=True)\n",
    "        slices_224.append(slice_224)\n",
    "        \n",
    "        # Preprocess to 128x128\n",
    "        slice_128 = preprocess_slice(slice_2d, target_size=(128, 128), normalize=False, to_rgb=True)\n",
    "        slices_128.append(slice_128)\n",
    "        \n",
    "        # Store label and metadata\n",
    "        slice_labels.append(label)\n",
    "        slice_metadata.append((mri_id, slice_idx))\n",
    "    \n",
    "    success_count += 1\n",
    "\n",
    "print(f\"\\nSlice extraction complete!\")\n",
    "print(f\"  Successful volumes: {success_count}\")\n",
    "print(f\"  Failed volumes: {failed_count}\")\n",
    "print(f\"  Total slices extracted (224x224): {len(slices_224)}\")\n",
    "print(f\"  Total slices extracted (128x128): {len(slices_128)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X_224 = np.array(slices_224, dtype=np.uint8)\n",
    "X_128 = np.array(slices_128, dtype=np.uint8)\n",
    "y = np.array(slice_labels, dtype=np.int32)\n",
    "\n",
    "print(f\"\\nArray shapes:\")\n",
    "print(f\"  X_224: {X_224.shape}\")\n",
    "print(f\"  X_128: {X_128.shape}\")\n",
    "print(f\"  y:     {y.shape}\")\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\nFinal class distribution:\")\n",
    "for class_idx, count in zip(unique, counts):\n",
    "    class_name = CONFIG['class_names'][class_idx]\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"  {class_name} ({class_idx}): {count} slices ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample slices from both classes\n",
    "plot_sample_slices(X_224, y, CONFIG['class_names'], num_samples=10, \n",
    "                   title=\"Sample MRI Slices (224x224)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified split (80% train, 20% test)\n",
    "train_split = CONFIG['train_split']\n",
    "random_state = CONFIG['random_state']\n",
    "\n",
    "# Split indices for both sizes\n",
    "indices = np.arange(len(y))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices, \n",
    "    test_size=1-train_split, \n",
    "    random_state=random_state, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Create train/test splits for 224x224\n",
    "X_train_224 = X_224[train_idx]\n",
    "X_test_224 = X_224[test_idx]\n",
    "\n",
    "# Create train/test splits for 128x128\n",
    "X_train_128 = X_128[train_idx]\n",
    "X_test_128 = X_128[test_idx]\n",
    "\n",
    "# Labels (same for both)\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "print(f\"Train-Test Split (stratified):\")\n",
    "print(f\"\\n224x224 images:\")\n",
    "print(f\"  Training set:   {X_train_224.shape}\")\n",
    "print(f\"  Test set:       {X_test_224.shape}\")\n",
    "print(f\"\\n128x128 images:\")\n",
    "print(f\"  Training set:   {X_train_128.shape}\")\n",
    "print(f\"  Test set:       {X_test_128.shape}\")\n",
    "print(f\"\\nLabels:\")\n",
    "print(f\"  Training set:   {y_train.shape}\")\n",
    "print(f\"  Test set:       {y_test.shape}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
    "for class_idx, count in zip(train_unique, train_counts):\n",
    "    class_name = CONFIG['class_names'][class_idx]\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"  {class_name} ({class_idx}): {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "test_unique, test_counts = np.unique(y_test, return_counts=True)\n",
    "for class_idx, count in zip(test_unique, test_counts):\n",
    "    class_name = CONFIG['class_names'][class_idx]\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    print(f\"  {class_name} ({class_idx}): {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to disk\n",
    "processed_data_path = CONFIG['processed_data_path']\n",
    "\n",
    "print(f\"Saving processed data to: {processed_data_path}\\n\")\n",
    "\n",
    "# Save 224x224 data\n",
    "np.save(processed_data_path / 'X_train_224.npy', X_train_224)\n",
    "np.save(processed_data_path / 'X_test_224.npy', X_test_224)\n",
    "print(f\"✓ Saved 224x224 images\")\n",
    "\n",
    "# Save 128x128 data\n",
    "np.save(processed_data_path / 'X_train_128.npy', X_train_128)\n",
    "np.save(processed_data_path / 'X_test_128.npy', X_test_128)\n",
    "print(f\"✓ Saved 128x128 images\")\n",
    "\n",
    "# Save labels\n",
    "np.save(processed_data_path / 'y_train.npy', y_train)\n",
    "np.save(processed_data_path / 'y_test.npy', y_test)\n",
    "print(f\"✓ Saved labels\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_slices': len(y),\n",
    "    'train_size': len(y_train),\n",
    "    'test_size': len(y_test),\n",
    "    'train_split': train_split,\n",
    "    'random_state': random_state,\n",
    "    'class_names': CONFIG['class_names'],\n",
    "    'train_indices': train_idx.tolist(),\n",
    "    'test_indices': test_idx.tolist(),\n",
    "    'class_distribution': {\n",
    "        'train': {int(k): int(v) for k, v in zip(train_unique, train_counts)},\n",
    "        'test': {int(k): int(v) for k, v in zip(test_unique, test_counts)}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(processed_data_path / 'dataset_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"✓ Saved metadata\")\n",
    "\n",
    "# Calculate and display file sizes\n",
    "print(f\"\\nFile sizes:\")\n",
    "for file_path in processed_data_path.glob('*.npy'):\n",
    "    size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {file_path.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\n✓ All data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPARATION - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n✓ Demographics loaded: {len(df_demographics)} records\")\n",
    "print(f\"✓ NIfTI volumes processed: {success_count}\")\n",
    "print(f\"✓ Total 2D slices extracted: {len(y)}\")\n",
    "print(f\"✓ Training samples: {len(y_train)} ({train_split*100:.0f}%)\")\n",
    "print(f\"✓ Test samples: {len(y_test)} ({(1-train_split)*100:.0f}%)\")\n",
    "print(f\"\\nClass Balance:\")\n",
    "for class_idx in [0, 1]:\n",
    "    class_name = CONFIG['class_names'][class_idx]\n",
    "    train_count = (y_train == class_idx).sum()\n",
    "    test_count = (y_test == class_idx).sum()\n",
    "    total_count = train_count + test_count\n",
    "    print(f\"  {class_name}: {total_count} total ({train_count} train, {test_count} test)\")\n",
    "\n",
    "print(f\"\\nData saved to: {processed_data_path.resolve()}\")\n",
    "print(f\"\\n✓ Ready for model training!\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  - Notebook 02: Train Model 1 (CNNs-without-Aug)\")\n",
    "print(f\"  - Notebook 03: Train Model 2 (CNNs-with-Aug)\")\n",
    "print(f\"  - Notebook 04: Train Model 3 (CNNs-LSTM-with-Aug) ⭐ BEST\")\n",
    "print(f\"  - Notebook 05: Train Model 4 (CNNs-SVM-with-Aug)\")\n",
    "print(f\"  - Notebook 06: Train Model 5 (VGG16-SVM-with-Aug)\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nvbygmmkkz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check what's happening with the path extraction\n",
    "sample_path = Path(r\"..\\Raw_Data\\OAS2_RAW_PART1\\OAS2_0001_MR1\\RAW\\mpr-1.nifti.hdr\")\n",
    "print(f\"Sample path: {sample_path}\")\n",
    "print(f\"Path parts: {sample_path.parts}\")\n",
    "\n",
    "# Try the function\n",
    "def extract_mri_id_from_path(file_path):\n",
    "    \"\"\"Extract MRI session ID (OAS2_XXXX_MRY) from file path.\"\"\"\n",
    "    parts = Path(file_path).parts\n",
    "    for part in parts:\n",
    "        if part.startswith('OAS2_'):\n",
    "            return part\n",
    "    return None\n",
    "\n",
    "mri_id = extract_mri_id_from_path(sample_path)\n",
    "print(f\"\\nExtracted MRI ID: {mri_id}\")\n",
    "\n",
    "# Check if it's in the label mapping\n",
    "print(f\"Is in label_mapping: {mri_id in label_mapping}\")\n",
    "print(f\"\\nFirst 5 keys in label_mapping:\")\n",
    "for key in list(label_mapping.keys())[:5]:\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ekfd2iiri8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the actual nifti_files to see what we're working with\n",
    "print(f\"Type of first nifti file: {type(nifti_files[0])}\")\n",
    "print(f\"First nifti file: {nifti_files[0]}\")\n",
    "print(f\"\\nParts: {nifti_files[0].parts}\")\n",
    "\n",
    "# Test extraction\n",
    "test_id = None\n",
    "for part in nifti_files[0].parts:\n",
    "    print(f\"Checking part: '{part}' - starts with OAS2_: {part.startswith('OAS2_')}\")\n",
    "    if part.startswith('OAS2_'):\n",
    "        test_id = part\n",
    "        break\n",
    "        \n",
    "print(f\"\\nExtracted ID: {test_id}\")\n",
    "print(f\"In label_mapping: {test_id in label_mapping if test_id else 'N/A'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
