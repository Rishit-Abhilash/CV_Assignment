{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model 3: CNNs-LSTM-with-Aug ⭐ BEST MODEL\n\n## 7-Layer CNN-LSTM Hybrid\n\n**Architecture:** TimeDistributed CNN + LSTM(100) + Dense(2)\n- Input: (1, 128, 128, 3) time-distributed\n- Epochs: 25, Batch: 16\n\n**Target:** 99.92% accuracy",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%run 00_utils_and_config.ipynb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load Data",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "X_train = np.load(CONFIG[\"processed_data_path\"] / \"X_train_128.npy\").astype(\"float32\") / 255.0\nX_test = np.load(CONFIG[\"processed_data_path\"] / \"X_test_128.npy\").astype(\"float32\") / 255.0\ny_train = np.load(CONFIG[\"processed_data_path\"] / \"y_train.npy\")\ny_test = np.load(CONFIG[\"processed_data_path\"] / \"y_test.npy\")\n\n# Reshape for LSTM: (samples, timesteps, height, width, channels)\n# Then convert to PyTorch format (samples, timesteps, channels, height, width)\nX_train = X_train.reshape((-1, 1, 128, 128, 3))\nX_test = X_test.reshape((-1, 1, 128, 128, 3))\n\n# Convert to PyTorch format: (N, T, C, H, W)\nX_train = np.transpose(X_train, (0, 1, 4, 2, 3))\nX_test = np.transpose(X_test, (0, 1, 4, 2, 3))\n\nprint(f\"Reshaped: {X_train.shape}, {X_test.shape}\")\n\n# Create tensors and dataloaders\nX_train_tensor = torch.from_numpy(X_train).float()\nX_test_tensor = torch.from_numpy(X_test).float()\ny_train_tensor = torch.from_numpy(y_train).long()\ny_test_tensor = torch.from_numpy(y_test).long()\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Build CNN-LSTM Model",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class CNNLSTM(nn.Module):\n    \"\"\"CNN-LSTM Hybrid for sequential MRI slice processing.\"\"\"\n    \n    def __init__(self, input_channels=3, lstm_hidden=100, num_classes=2):\n        super(CNNLSTM, self).__init__()\n        \n        # CNN layers (applied to each timestep)\n        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # After 128x128 -> 64x64 -> 32x32\n        self.flatten_size = 32 * 32 * 32\n        \n        # LSTM layer\n        self.lstm = nn.LSTM(self.flatten_size, lstm_hidden, batch_first=True)\n        \n        # Output layer\n        self.fc = nn.Linear(lstm_hidden, num_classes)\n    \n    def forward(self, x):\n        # x shape: (batch, timesteps, channels, height, width)\n        batch_size, timesteps, C, H, W = x.size()\n        \n        # Process each timestep through CNN\n        c_out = []\n        for t in range(timesteps):\n            # CNN for this timestep\n            c = F.relu(self.conv1(x[:, t, :, :, :]))\n            c = self.pool1(c)\n            c = F.relu(self.conv2(c))\n            c = self.pool2(c)\n            c = c.view(batch_size, -1)  # Flatten\n            c_out.append(c)\n        \n        # Stack timesteps\n        lstm_input = torch.stack(c_out, dim=1)  # (batch, timesteps, features)\n        \n        # LSTM\n        lstm_out, _ = self.lstm(lstm_input)\n        \n        # Take last timestep output\n        last_output = lstm_out[:, -1, :]\n        \n        # Output\n        out = self.fc(last_output)\n        return out\n\nmodel = CNNLSTM(input_channels=3, lstm_hidden=100, num_classes=2).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nprint(model)\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Train",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "start_time = time.time()\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\nbest_val_acc = 0.0\nbest_model_path = str(CONFIG[\"saved_models_path\"] / \"model3_cnn_lstm_best.pth\")\n\n# Split for validation\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\ntrain_loader_split = DataLoader(train_subset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_subset, batch_size=16, shuffle=False)\n\nfor epoch in range(25):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for inputs, labels in train_loader_split:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_loss = running_loss / total\n    train_acc = correct / total\n    \n    # Validate\n    model.eval()\n    val_loss, val_correct, val_total = 0.0, 0, 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n    \n    val_loss /= val_total\n    val_acc = val_correct / val_total\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Epoch {epoch+1}/25 - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} ⭐ BEST\")\n    else:\n        print(f\"Epoch {epoch+1}/25 - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\ntraining_time = time.time() - start_time\nprint(f\"\\n✓ Training complete - Best val acc: {best_val_acc*100:.2f}%\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Evaluate",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "best_model = CNNLSTM(input_channels=3, lstm_hidden=100, num_classes=2).to(device)\nbest_model.load_state_dict(torch.load(best_model_path))\nbest_model.eval()\n\nstart_time = time.time()\nall_preds, all_probs = [], []\nwith torch.no_grad():\n    for inputs, _ in test_loader:\n        inputs = inputs.to(device)\n        outputs = best_model(inputs)\n        probs = F.softmax(outputs, dim=1)\n        all_probs.append(probs.cpu().numpy())\n        _, predicted = torch.max(outputs.data, 1)\n        all_preds.append(predicted.cpu().numpy())\n\ny_pred_proba = np.vstack(all_probs)\ny_pred = np.concatenate(all_preds)\ntesting_time = (time.time() - start_time) * 1000\n\nmetrics = calculate_all_metrics(y_test, y_pred, y_pred_proba[:, 1])\nprint_metrics(metrics, \"Model 3: CNN-LSTM (BEST)\")\nprint(f\"⭐ Accuracy: {metrics['accuracy']*100:.2f}% (Target: 99.92%)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Save",
   "execution_count": null,
   "outputs": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "plot_training_history(history, \"Model 3: CNN-LSTM\", CONFIG[\"results_path\"] / \"training_curves\" / \"model3_training.png\")\ntorch.save(model.state_dict(), CONFIG[\"saved_models_path\"] / \"model3_cnn_lstm_final.pth\")\n\nresults = {\"model_name\": \"CNN-LSTM-with-Aug\", \"accuracy\": float(metrics[\"accuracy\"]), \"precision\": float(metrics[\"precision\"]), \"recall\": float(metrics[\"recall\"]), \"f1_score\": float(metrics[\"f1_score\"]), \"specificity\": float(metrics[\"specificity\"]), \"training_time_seconds\": float(training_time), \"testing_time_ms\": float(testing_time)}\n\nwith open(CONFIG[\"results_path\"] / \"model3_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"\\n✓ Model 3 (BEST) saved - Accuracy: {metrics['accuracy']*100:.2f}%\")",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}